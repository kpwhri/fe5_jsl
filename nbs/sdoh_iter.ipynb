{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Social Determinants of Health for FE5\n",
    "\n",
    "This notebook details using JohnSnowLabs models to extract SDOH concepts from clinical text. \n",
    "\n",
    "This notebook will iterate through N records at a time to avoid reaching computer memory limits.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "The following was run on a Windows VM and so some configurations/setup will be different.\n",
    "\n",
    "Windows prerequisites:\n",
    "* Download relevant version of hadoop/bin to, e.g., `C:\\hadoop\\bin` from https://github.com/cdarlint/winutils \n",
    "* Environment variables (these have been included in the following code blocks):\n",
    "    * `HADOOP_HOME`= `C:\\hadoop`\n",
    "    * `PATH` += `%HADOOP_HOME%\\bin`\n",
    "\n",
    "Prerequisites:\n",
    "* Java (JDK 1.8/Java 8)\n",
    "    * Set `JAVA_HOME` to install path, and add to `PATH`\n",
    "* Python version compatible with SparkNLP\n",
    "    * Python packages: see `my.johnsnowlabs.com/docs` > `Install Locally on Python` > follow steps \n",
    "        * Some of these installs will take quite a while to download relevant jar files.\n",
    "* Download license keys from `https://my.johnsnowlabs.com/subscriptions`\n",
    "    * Save json file to, e.g., `C:\\spark_jsl_54.json`\n",
    "    * NB: Different versions of Spark NLP (e.g., 5.3 vs 5.4) will require different license keys.  \n",
    "\n",
    "## Input Data\n",
    "\n",
    "The input data should be either a CSV file or a JSONL file.\n",
    "\n",
    "The CSV file should have the following structure (the names can be different, but these are preferred):\n",
    "* `note_id`: an arbitrary note_id; this should be unique across the dataset (order won't work due to concurrency)\n",
    "* `note_text`: text to be processed; null values will be dropped\n",
    "\n",
    "## Output Data\n",
    "\n",
    "This notebook will create a CSV file with the following variables:\n",
    "* `note_id`: note_id from input data \n",
    "* `confidence`: float in range (0.0, 1.0]; some cutoff should be decided\n",
    "* `entity`: SDOH entity, one of the following:\n",
    "    * 'Access_To_Care'\n",
    "    * 'Chidhood_Event'\n",
    "    * 'Community_Safety'\n",
    "    * 'Disability'\n",
    "    * 'Eating_Disorder'\n",
    "    * 'Education'\n",
    "    * 'Environmental_Condition'\n",
    "    * 'Exercise'\n",
    "    * 'Family_Member'\n",
    "    * 'Financial_Status'\n",
    "    * 'Food_Insecurity'\n",
    "    * 'Geographic_Entity'\n",
    "    * 'Healthcare_Institution'\n",
    "    * 'Housing'\n",
    "    * 'Income'\n",
    "    * 'Insurance_Status'\n",
    "    * 'Legal_Issues'\n",
    "    * 'Mental_Health'\n",
    "    * 'Other_SDoH_Keywords'\n",
    "    * 'Population_Group'\n",
    "    * 'Quality_Of_Life'\n",
    "    * 'Social_Exclusion'\n",
    "    * 'Social_Support'\n",
    "    * 'Spiritual_Beliefs'\n",
    "    * 'Substance_Duration'\n",
    "    * 'Substance_Frequency'\n",
    "    * 'Substance_Quantity'\n",
    "    * 'Substance_Use'\n",
    "    * 'Transportation'\n",
    "    * 'Violence_Or_Abuse'\n",
    "* `assertion`: descriptors of SDOH concept\n",
    "    * `Present`: is current\n",
    "    * `Past`: described in the past\n",
    "    * `Someone_Else`: related to a different person\n",
    "    * `Possible`: described as possible\n",
    "    * `Absent`: negated\n",
    "    * `Family_History`: related to family history\n",
    "    * Others?\n"
   ],
   "id": "aa9d27e22ae4f836"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from johnsnowlabs import nlp, medical\n",
    "from loguru import logger\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.window import Window\n",
    "import sparknlp\n",
    "import sparknlp_jsl"
   ],
   "id": "e6af45e5fec4b77b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load license keys, prepare configurations\n",
    "with open(r'C:\\spark_jsl_54.json') as f:\n",
    "    license_keys = json.load(f)\n",
    "\n",
    "locals().update(license_keys)\n",
    "os.environ.update(license_keys)\n",
    "hadoop_home = r'C:\\hadoop'\n",
    "os.environ['HADOOP_HOME'] = hadoop_home\n",
    "os.environ['PATH'] += fr';{hadoop_home}\\bin;{hadoop_home}'\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "params = {'spark.driver.memory': '4G',\n",
    "          'spark.executor.memory': '19G',\n",
    "          'spark.kryoserializer.buffer.max': '2000M',\n",
    "          'spark.driver.maxResultSize': '2000M',\n",
    "          }\n",
    "\n",
    "spark = sparknlp_jsl.start(license_keys['SECRET'], gpu=False, params=params)\n",
    "\n",
    "logger.info(f'Spark NLP Version: {sparknlp.version()}')\n",
    "logger.info(f'Spark NLP_JSL Version: {sparknlp_jsl.version()}')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# configurations\n",
    "note_id_col = 'note_id'\n",
    "note_text_col = 'note_text'\n",
    "in_dataset_path = Path(r'path/to/example.jsonl')\n",
    "out_dataset_path = Path(r'path/to/out.csv')\n",
    "test_top_n = None  # set to integer to only run on e.g., 'top 100' for testing purposes\n",
    "batch_size = 1000  # to determine optimal batch size, consider running `sdoh.ipynb` with varying `test_top_n`\n",
    "# logger.add('logfile')  # uncomment this to log to file"
   ],
   "id": "6953e30640737360"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# the pipeline\n",
    "documentAssembler = (\n",
    "    nlp.DocumentAssembler()\n",
    "    .setInputCol('note_text')\n",
    "    .setIdCol('note_id')\n",
    "    .setOutputCol('document')\n",
    ")\n",
    "\n",
    "sentenceDetector = (\n",
    "    nlp.SentenceDetectorDLModel.pretrained('sentence_detector_dl_healthcare', 'en', 'clinical/models')\n",
    "    .setInputCols(['document'])\n",
    "    .setOutputCol('sentence').setCustomBounds(['\\|'])\n",
    ")\n",
    "\n",
    "tokenizer = nlp.Tokenizer() \\\n",
    "    .setInputCols(['sentence']) \\\n",
    "    .setOutputCol('token')  #\\\n",
    "\n",
    "clinical_embeddings = nlp.WordEmbeddingsModel.pretrained('embeddings_clinical', 'en', 'clinical/models') \\\n",
    "    .setInputCols(['sentence', 'token']) \\\n",
    "    .setOutputCol('embeddings')\n",
    "\n",
    "ner_model = medical.NerModel.pretrained('ner_sdoh', 'en', 'clinical/models') \\\n",
    "    .setInputCols(['sentence', 'token', 'embeddings']) \\\n",
    "    .setOutputCol('ner')\n",
    "\n",
    "ner_conv = medical.NerConverterInternal() \\\n",
    "    .setInputCols(['sentence', 'ner', 'token']) \\\n",
    "    .setOutputCol('chunk_main')\n",
    "\n",
    "assertion = medical.AssertionDLModel.pretrained('assertion_sdoh_wip', 'en', 'clinical/models') \\\n",
    "    .setInputCols(['sentence', 'token', 'embeddings', 'chunk_main']) \\\n",
    "    .setOutputCol('assertion')\n",
    "\n",
    "assertion_filterer_hypo = medical.AssertionFilterer() \\\n",
    "    .setInputCols(['sentence', 'chunk_main', 'assertion']) \\\n",
    "    .setOutputCol('filtered') \\\n",
    "    .setCriteria('assertion') \\\n",
    "    .setWhiteList(['present', 'Possible', 'Absent', 'Family_History', 'Someone_Else', 'Past'])\n",
    "\n",
    "assertion_filterer_present = medical.AssertionFilterer() \\\n",
    "    .setInputCols(['sentence', 'chunk_main', 'assertion']) \\\n",
    "    .setOutputCol('filtered_present') \\\n",
    "    .setCriteria('assertion') \\\n",
    "    .setWhiteList(['present', 'Planned'])\n",
    "\n",
    "assertion_filterer_possible = medical.AssertionFilterer() \\\n",
    "    .setInputCols(['sentence', 'chunk_main', 'assertion']) \\\n",
    "    .setOutputCol('filtered_possible') \\\n",
    "    .setCriteria('assertion') \\\n",
    "    .setWhiteList(['Possible'])\n",
    "\n",
    "assertion_filterer_absent = medical.AssertionFilterer() \\\n",
    "    .setInputCols(['sentence', 'chunk_main', 'assertion']) \\\n",
    "    .setOutputCol('filtered_absent') \\\n",
    "    .setCriteria('assertion') \\\n",
    "    .setWhiteList(['Absent'])\n",
    "\n",
    "assertion_filterer_hist = medical.AssertionFilterer() \\\n",
    "    .setInputCols(['sentence', 'chunk_main', 'assertion']) \\\n",
    "    .setOutputCol('filtered_hist') \\\n",
    "    .setCriteria('assertion') \\\n",
    "    .setWhiteList(['Family_History', 'Someone_Else'])\n",
    "\n",
    "assertion_filterer_past = medical.AssertionFilterer() \\\n",
    "    .setInputCols(['sentence', 'chunk_main', 'assertion']) \\\n",
    "    .setOutputCol('filtered_past') \\\n",
    "    .setCriteria('assertion') \\\n",
    "    .setWhiteList(['Past'])\n",
    "\n",
    "jsl_single_pipeline = nlp.Pipeline(\n",
    "    stages=[\n",
    "        documentAssembler,\n",
    "        sentenceDetector,\n",
    "        tokenizer,\n",
    "        clinical_embeddings,\n",
    "        ner_model,\n",
    "        ner_conv,\n",
    "        assertion,\n",
    "        assertion_filterer_hypo,\n",
    "        assertion_filterer_present,\n",
    "        assertion_filterer_possible,\n",
    "        assertion_filterer_absent,\n",
    "        assertion_filterer_hist,\n",
    "        assertion_filterer_past\n",
    "    ]\n",
    ")"
   ],
   "id": "fae7e85a58e87094"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# read dataset\n",
    "if in_dataset_path.suffix == '.jsonl':\n",
    "    df = spark.read.json(str(in_dataset_path)).select(note_id_col, note_text_col)\n",
    "elif in_dataset_path.suffix == '.csv':\n",
    "    df = spark.read.csv(str(in_dataset_path), header=True).select(note_id_col, note_text_col)\n",
    "else:\n",
    "    raise ValueError(f'Unrecognized extension: {in_dataset_path.suffix}')\n",
    "\n",
    "# prepare dataset for processing\n",
    "df = df.na.drop()  # drop notes without text\n",
    "df = df.toDF('note_id', 'note_text')  # rename columns"
   ],
   "id": "9b4923bc8a369841"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# experiment with just a few records\n",
    "if test_top_n:\n",
    "    df = df.limit(test_top_n)"
   ],
   "id": "a7793cd5253b6c24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# prepare pipelines\n",
    "p_model = jsl_single_pipeline.fit(df)\n",
    "l_model = nlp.LightPipeline(p_model)"
   ],
   "id": "e75104517eb970a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def annotate(row):\n",
    "    \"\"\" Annotates the data provided into `Annotation` type results. \"\"\"\n",
    "    note_id = row['note_id']\n",
    "    data = l_model.fullAnnotate(row['note_text'])\n",
    "    for result in data:\n",
    "        for entity in result['filtered']:\n",
    "            entity = entity.metadata\n",
    "            ner_type = entity['entity']\n",
    "            assertion = entity['assertion']\n",
    "            yield {\n",
    "                'note_id': note_id,\n",
    "                'confidence': entity['confidence'],\n",
    "                'assertion': assertion,\n",
    "                'entity': ner_type,\n",
    "            }"
   ],
   "id": "e0e54ff8ee7612ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# prepare batches for processing dataframe \n",
    "window_spec = Window.orderBy('note_id')  # Replace \"some_column\" with a column to order by\n",
    "df = df.withColumn('row_num', row_number().over(window_spec))\n",
    "num_batches = (df.count() + batch_size - 1) // batch_size\n",
    "logger.info(f'Number of batches: {num_batches}')"
   ],
   "id": "1a8f6e9e661dbd9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# process write output to CSV\n",
    "before_time = time.time()\n",
    "n_entities = 0\n",
    "n_notes = 0\n",
    "fieldnames = ['note_id', 'confidence', 'entity', 'assertion']\n",
    "with open(out_dataset_path, 'w', newline='', encoding='utf8') as fh:\n",
    "    writer = csv.DictWriter(fh, fieldnames)\n",
    "    writer.writeheader()\n",
    "    for batch_num in range(num_batches):\n",
    "        logger.info(f'Starting batch #{batch_num + 1} of {num_batches}')\n",
    "        # get the current batch dataframe\n",
    "        start_row = batch_num * batch_size + 1\n",
    "        end_row = (batch_num + 1) * batch_size\n",
    "        batch_df = df.filter((df.row_num >= start_row) & (df.row_num <= end_row))\n",
    "        for i, row in enumerate(batch_df.collect(), start=n_notes + 1):\n",
    "            for result in annotate(row):\n",
    "                # result: {'note_id': '1', 'confidence': 0.9981, 'assertion': 'Present', 'entity': 'Employment'}\n",
    "                writer.writerow(result)\n",
    "                n_entities += 1\n",
    "            if i % 10_000 == 0:\n",
    "                after_time = time.time()\n",
    "                logger.info(f'Processed {i} notes and wrote {n_entities} entities to file; {(after_time - before_time)/i} s/notes')\n",
    "        n_notes += batch_df.count()\n",
    "        after_time = time.time()\n",
    "        logger.info(f'Finished batch {batch_num + 1}! Processed {n_notes} notes and wrote {n_entities} entities to file; {(after_time - before_time)/n_notes} s/notes')\n",
    "after_time = time.time()\n",
    "logger.info(f'Done! Processed {n_notes} notes and wrote {n_entities} entities to file; {(after_time - before_time)/n_notes} s/notes')"
   ],
   "id": "8ffe0f83df3a6c8d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
